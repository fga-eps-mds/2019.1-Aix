---
layout: post
title: Getting Started Scrapy
tags: webcrawler treinamento
category: Projeto
---
|Data   |Versão   |Descrição   |Autor   |
|---|---|---|---|
|09/05/2019   | 1.0  |Criação do documento   |Lucas Vitor   |

## Introdução

Scrapy é um framework open source para web crawler e web scraping, ou seja, para navegar na internet de forma metódica e automatizada e para coletar dados da web.

## Instalação

O guia de instalação completo pode ser encontrado na documentação do próprio scrapy. Disponível em:

```
https://docs.scrapy.org/en/latest/intro/install.html
```

Em resumo, o scrapy pode ser instalado através dos pacotes de python com o seguinte comando, utiizando o pip (para seguir o tutorial aqui presente, recomenda-se a utilização de Python 3.4 ou superior):

```
pip install Scrapy
```

## Criando o seu primeiro projeto no scrapy

### Setando o projeto

Partindo do pressuposto de que você já possui o scrapy instalado em sua máquina (caso não tenha, volte na sessão Instalação), você tem duas opções para a criação do seu projeto:

<ol>
    <li> Crie um arquivo com a extensão .py no diretório de sua preferência.</li>

    <li> Digite o seguinte comando: </li>
</ol>

```
scrapy startproject <project_name>
```

Após este comando será gerado um diretório da seguinte forma:

![scrapy tree]({{ site.url }}/2019.1-Aix/assets/img/scrapy_tree.png)

Vá na pasta spiders e crie um arquivo .py

**Obs:** A diferença principal entre as duas opções se dá pela organização do seu projeto (recomenda-se a utilização da segunda opção para qualquer projeto que pretenda ser continuado)


### Codando seu spider

Essa parte do tutorial independe da opção escolhida no tópico anterior:

1. Em seu documento criado import o scrapy:

```python
import scrapy
```

2. Crie um classe que herda de scrapy.Spider, e crie um atributo name e atribua a ele um nome:

```python
class QuotesSpider(scrapy.Spider):
    name = "quotes"
```

3. Crie um método chamado start_requests, que possuirá as urls para o spider. o método deverá ter a seguinte estrutura:

```python
def start_requests(self):
    url = "http://quotes.toscrape.com/page/1/"
    yield scrapy.Request(url=url, callback=self.parse)
```

**Obs:** Esse método é opcional, você pode criar um atributo da classe da seguinte forma:

```python
start_urls = [
    'http://quotes.toscrape.com/page/1/',
]
```

E este servirá da mesma forma.


4. Crie um método chamado parse, ele será o responsável por receber a response do request e transformá-la da forma que desejar. Este método para este exemplo, ele seguirá a seguinte estrutura:

```python
def parse(self, response):
    for quote in response.css('div.quote'):
        yield {
            'text': quote.css('span.text::text').get(),
            'author': quote.css('small.author::text').get(),
            'tags': quote.css('div.tags a.tag::text').getall(),
        }

    next_page = response.css('li.next a::attr(href)').get()
    if next_page is not None:
        next_page = response.urljoin(next_page)
        yield scrapy.Request(next_page, callback=self.parse)
```

E pronto, seu primeiro spider está criado!


### Rodando seu spider

1. Caso você tenha escolhido a primeira opção no passo "Setando o projeto", vá no diretório do seu arquivo .py, e digite o seguinte comando:

```
scrapy runspider <nome_do_seu_arquivo>
```

E caso queira gerar um json, por exemplo, digite o seguinte comando:

```
scrapy runspider <nome_do_seu_arquivo> -o <arquivo_de_saida>.json
```

2. Caso tenha escolhido a segunda opção, vá no diretório raíz do seu projeto e digite o seguinte comando:

```
scrapy crawl <nome_do_seu_arquivo>
```

E caso queira gerar um json, por exemplo, digite o seguinte comando:

```
scrapy crawl <nome_do_seu_arquivo> -o <arquivo_de_saida>.json
```


## Acessando os dados coletados

Para ter acesso aos dados recolhidos pelo scrapy, existem várias opções. Para o presente projeto, as melhores opções são: (a) criar um banco de dados (de preferência não relacional) para armazenar os dados e requisitá-los sempre que necessário, (b) criar um servidor para responder através de requisições HTTP.

Aqui será tratada a segunda opção.

### Scrapyrt

Intalando o scrapyrt:

```
pip install scrapyrt
```

Utilizando:

1. Vá no diretório do seu projeto scrapy

2. Digite o o seguinte comando:

```
scrapyrt -p <PORT>
```

3. Abra seu navegador e digite a seguinte URL:

```
http://localhost:<PORT>/crawl.json?start_requests=true&spider_name=<SPIDER_NAME>
```